# Desenvolvimento de um Pipeline de Dados em Python dedicado √† extra√ß√£o de informa√ß√µes atrav√©s de uma API. Utilizando as poderosas ferramentas Apache Airflow para orquestra√ß√£o do fluxo de dados e Apache Spark para realizar extra√ß√£o e transforma√ß√µes robustas nos conjuntos de dados.

## üí°Objetivos
O objetivo deste projeto √© aprimorar o projeto "Desenvolvimento de um Pipeline de Dados em Python para extra√ß√£o de dados via API utilizando o Apache Airflow" [https://github.com/alexcmendonca/apache-airflow-pipeline-api-datalake]. Esta atualiza√ß√£o incorpora a ferramenta Spark para aprimorar a transforma√ß√£o dos dados, introduzindo uma nova estrutura ao Data Lake com a implementa√ß√£o de uma arquitetura de medalhas. Ao t√©rmino do projeto, os dados ser√£o entregues de maneira organizada, facilitando sua interpreta√ß√£o.

###### Imagem 1: Diagrama final do Pipeline deste Projeto (Cr√©ditos da imagem: Alura)
<img src="/assets/img/img-pipeline-final.png">


## üñ•Ô∏èDesafios do Projeto
Empregar a integra√ß√£o do Apache Airflow com Spark, PySpark e o operador Spark Submit para efetuar a transforma√ß√£o dos dados e promover a reestrutura√ß√£o do Data Lake por meio de uma arquitetura baseada em medalhas. Esse enfoque fragmentar√° as etapas em distintas camadas, configurando as tr√™s camadas como a nova estrutura aplicada ao Data Lake. Essa abordagem n√£o apenas proporcionar√° maior organiza√ß√£o ao ambiente, mas tamb√©m viabilizar√° a refinamento dos dados brutos de maneira mais eficiente.

###### Imagem 2: Diagrama final do Pipeline deste Projeto (Cr√©ditos da imagem: Alura)
<img src="/assets/img/arquitetura_medalhao.png">

**- Resumo da execu√ß√£o do Projeto**

Reestrutura√ß√£o do Data Lake adotando uma arquitetura de medalh√£o composta por tr√™s camadas: Bronze, Silver e Gold. Essa abordagem visa conferir maior organiza√ß√£o ao Data Lake, proporcionando suporte eficaz para o refinamento dos dados brutos.

A explora√ß√£o detalhada dos dados ocorreu por meio do Spark / PySpark, utilizando o arquivo Jupyter Notebook denominado "exploracao_spark.ipynb". Durante a an√°lise dos dados, foram identificadas as informa√ß√µes essenciais e aquelas que deveriam ser descartadas. Com base nessas conclus√µes, procedemos √† separa√ß√£o dos dados relevantes, simplificando a interpreta√ß√£o dos conjuntos de dados. A aplica√ß√£o dos m√©todos select e explode possibilitou a extra√ß√£o precisa dos dados relacionados a tweets e usu√°rios, sendo posteriormente armazenados em um novo DataFrame.

Transformamos a explora√ß√£o de dados realizada no Jupyter Notebook em um script Python denominado "transformation.py", o qual ser√° posteriormente integrado ao pipeline orquestrado pelo Apache Airflow.
	Fun√ß√µes desenvolvidas:
		**get_tweets_data():** Encarregada de extrair os dados brutos do DataFrame e criar um novo contendo exclusivamente os tweets.
		get_users_data(df): Utilizada para extrair informa√ß√µes relacionadas aos usu√°rios.
		export_json(): Respons√°vel por salvar e armazenar os dados resultantes.
		twitter_transformation(): Fun√ß√£o principal que incorpora Spark, englobando todas as fun√ß√µes. Realiza a leitura do DataFrame original e invoca as fun√ß√µes de extra√ß√£o e salvamento, consolidando assim a transforma√ß√£o dos dados.

Testamos o script Python "transformation.py" utilizando o Spark Submit.

Instala√ß√£o e utiliza√ß√£o do Operador Spark, especificamente o "SparkSubmitOperator", na atualiza√ß√£o do arquivo de Dags, denominado "api_dag.py". Nessa atualiza√ß√£o, foi criado um novo operador dedicado √† transforma√ß√£o dos dados e √† grava√ß√£o dos resultados.

Para estabelecer a conex√£o entre o Apache Airflow e o Spark, editamos a conex√£o preexistente chamada "spark_default". Esse ajuste foi realizado por meio da interface web do Airflow, acess√≠vel pela se√ß√£o "Admin" na barra superior, selecionando a op√ß√£o "Connections".

Executamos o Apache Airflow, agora com a DAG devidamente atualizada, o que resultou na grava√ß√£o dos dados transformados no diret√≥rio "dados_transformation".

Aprimorando o Projeto com a Reestrutura√ß√£o do Data Lake e Refatora√ß√£o do C√≥digo da DAG.

	1. Reorganiza√ß√£o do Data Lake em Duas Camadas (Bronze e Silver): Para efetuar essa modifica√ß√£o, procedemos com a atualiza√ß√£o da Directed Acyclic Graph (DAG), corrigindo caminhos e pastas para alinhar-se com os objetivos do projeto.

	2. Aprimoramento da Dinamicidade e Centraliza√ß√£o da Estrutura do Data Lake: Implementamos medidas que garantem a dinamicidade e centraliza√ß√£o da estrutura do Data Lake. Para isso, introduzimos duas vari√°veis cruciais:
                - BASE_FOLDER: Utilizada como refer√™ncia para o caminho do Data Lake, tornando desnecess√°rio lembrar de todos os pontos que precisam ser alterados em caso de modifica√ß√£o.
                - PARTITION_FOLDER_EXTRACT: Respons√°vel por retornar a data de in√≠cio da extra√ß√£o (extract_date), proporcionando uma abordagem mais eficiente.
        
        Essas atualiza√ß√µes visam otimizar a manuten√ß√£o e evolu√ß√£o cont√≠nua do projeto, conferindo-lhe maior flexibilidade e praticidade.

Inclus√£o de nova demanda no projeto: adicionando uma nova etapa no pipeline. Al√©m do desenvolvimento da extra√ß√£o de dados, implementamos uma fase adicional para realizar a transforma√ß√£o desses dados. Essa transforma√ß√£o tem como objetivo gerar um novo conjunto de dados que ir√° apoiar o desenvolvimento de um relat√≥rio espec√≠fico. Nessa nova etapa, o foco foi na extra√ß√£o dos valores relacionados √† data em que os tweets foram postados.

Essa abordagem permitiu visualizar o desempenho da palavra-chave "data science" nos tweets de forma mais espec√≠fica e detalhada. Resumindo, a ideia foi utilizar os dados j√° coletados at√© o momento na etapa de extra√ß√£o e, agora, aplicando uma transforma√ß√£o, consolidando essas informa√ß√µes por dia da semana. Essa estrat√©gia proporcionar√° uma an√°lise mais refinada e direcionada ao aspecto temporal, otimizando a gera√ß√£o de insights para o desenvolvimento do relat√≥rio.

Para atender essa demanda, foi realizado os passos semelhantes aos realizados na camada Silver:

        Explora√ß√£o dos Dados na Camada Silver: Iniciamos o processo explorat√≥rio dos dados contidos na camada Silver.

        Cria√ß√£o de um Script de Transforma√ß√£o de Dados: Desenvolvemos um script dedicado √† transforma√ß√£o dos dados, visando aprimorar a qualidade e relev√¢ncia das informa√ß√µes.

        Integra√ß√£o da Nova Etapa no Pipeline de Dados: Adicionamos com sucesso essa nova etapa ao nosso pipeline de dados, assegurando a fluidez e efic√°cia do processo de transforma√ß√£o e an√°lise.


## üìÑDesenvolvimento de Compet√™ncias:
|Atividades|Realizadas |
|----------|-----------|
| Compreender como funciona a arquitetura de medalhas | Explorar dados utilizando o PySpark
| Transformar dados utilizando o PySpark | Extrair dados utilizando o PySpark
| Salvar dados utilizando o PySpark | Executar scripts utilizando o SparkSubmit
| Instalar o modulo Spark | Utilizar o SparkSubmitOperator
| Atualizar uma DAG | Salvar os dados transformados pelo Spark
| Centralizar nossas pastas em um vari√°vel | Estruturar nosso data lake
| Reestruturar nosso c√≥digo | Refinar informa√ß√µes para a camada Gold
| Utilizar um operador para transformar dados da camada Gold | Atualizar a DAG com a nova etapa de transforma√ß√£o
| Explorar os dados da camada Gold | |

## üóÇÔ∏èOrganiza√ß√£o dos Arquivos
- Dados_transformation: Grava√ß√£o de dados na etapa
- Datalake: Reposit√≥rio Data Lake com arquitetura de medalh√µes - Camadas: Bronze, Silver e Gold, proporcionando maior organiza√ß√£o ao Data Lake
- Hook: Implementa√ß√£o de um Hook no Airflow para realizar a conex√£o com a API utilizando o HTTP Hook. Esse Hook √© utilizado para efetuar as requisi√ß√µes, utilizando a configura√ß√£o de uma conex√£o no Airflow
- Operators: Desenvolvimento de um operador para extrair dados da API, criando um Data Lake e armazenando os dados nesse reposit√≥rio
- Spark-3.1.3: Cont√©m a distribui√ß√£o bin√°ria do Apache Spark na vers√£o 3.1.3, essa pasta √© crucial para garantir que o ambiente Spark esteja corretamente configurado em seu sistema
 - SRC: Armazena o c√≥digo fonte do projeto | Subpastas:
	Notebooks: Notebooks Jupyter, que cont√™m c√≥digos, gr√°ficos e coment√°rios relacionados √† an√°lise e explora√ß√£o de dados. Os Notebooks Jupyter s√£o usados para executar c√≥digo de forma interativa e s√£o populares no ambiente de ci√™ncia de dados.
	Scripts: c√≥digo que realiza a extra√ß√£o de dados de uma API.
	Spark: Scripts Python que utiliza o Apache Spark para realizar transforma√ß√µes, limpeza, manipul√ß√£o e agrega√ß√£o de dados usando a capacidade de processamento distribu√≠do do Spark
- DAGs: Automa√ß√£o do pipeline realizada por meio do DAG, no qual s√£o especificadas as instru√ß√µes para o Airflow sobre como a extra√ß√£o do operador ser√° executada. Define-se par√¢metros como frequ√™ncia, data inicial e data final.

## üéûÔ∏èImagens do Projeto

###### Imagem 3: Interface Airflow - Visualiza√ß√£o DAG e sua execu√ß√£o
<img src="/assets/img/img-execucao-dag.png">

###### Imagem 4: Visualiza√ß√£o gr√°fica de um DAG e suas depend√™ncias
<img src="/assets/img/img-grafico-dag.png">

###### Imagem 5: Visualiza√ß√£o do calend√°rio
<img src="/assets/img/img-calendario-dag.png">


## üîçRefer√™ncias
- [Alura](https://www.alura.com.br/)
