{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.3.1\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 8.1 kB/s eta 0:00:01    |████████▋                       | 75.4 MB 6.9 MB/s eta 0:00:31     |███████████▋                    | 101.8 MB 4.5 MB/s eta 0:00:40     |██████████████████              | 159.1 MB 6.6 MB/s eta 0:00:19     |██████████████████████          | 194.0 MB 6.6 MB/s eta 0:00:14     |██████████████████████▉         | 201.1 MB 3.0 MB/s eta 0:00:27     |████████████████████████        | 210.3 MB 5.9 MB/s eta 0:00:13     |█████████████████████████▏      | 221.0 MB 6.9 MB/s eta 0:00:09     |██████████████████████████▋     | 234.1 MB 3.7 MB/s eta 0:00:13     |███████████████████████████     | 237.8 MB 4.0 MB/s eta 0:00:11     |███████████████████████████▌    | 241.7 MB 2.7 MB/s eta 0:00:15     |███████████████████████████▉    | 245.0 MB 2.8 MB/s eta 0:00:13     |████████████████████████████▌   | 250.7 MB 4.2 MB/s eta 0:00:08     |████████████████████████████▊   | 252.4 MB 4.2 MB/s eta 0:00:07     |█████████████████████████████▋  | 260.1 MB 913 kB/s eta 0:00:24     |█████████████████████████████▋  | 260.4 MB 913 kB/s eta 0:00:23     |██████████████████████████████  | 263.3 MB 3.3 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/alex/Documents/airflow/venv/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-fwjrmv83/pyspark/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-fwjrmv83/pyspark/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-4k0xfy7z\n",
      "       cwd: /tmp/pip-install-fwjrmv83/pyspark/\n",
      "  Complete output (6 lines):\n",
      "  usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "     or: setup.py --help [cmd1 cmd2 ...]\n",
      "     or: setup.py --help-commands\n",
      "     or: setup.py cmd --help\n",
      "  \n",
      "  error: invalid command 'bdist_wheel'\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for pyspark\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for pyspark\n",
      "Failed to build pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "    Running setup.py install for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# instalação do pyspark\n",
    "%pip install pyspark==3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando classe SparkSession ponto de entrada ler dados, consultas SQL\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/12 15:20:35 WARN Utils: Your hostname, alex-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.112.128 instead (on interface ens33)\n",
      "24/01/12 15:20:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/12 15:20:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# criando uma instância da classe 'SparkSession' no ambiente PySpark\n",
    "# iniciando a construção da sessão Spark, definindo nome da aplicação\n",
    "# e retornando a SparkSession existente ou cria uma nova se não existir\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('api_transformation')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Definindo df de em um arquivo JSON\n",
    "df = spark.read.json('../../datalake/api_datascience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------------+------------+\n",
      "|     _corrupt_record|                data|            includes|              meta|extract_date|\n",
      "+--------------------+--------------------+--------------------+------------------+------------+\n",
      "|                null|[{74, 16, 2024-01...|{[{2024-01-07T06:...|{1234567890abcdef}|  2024-01-07|\n",
      "|                null|[{14, 77, 2024-01...|{[{2024-01-07T16:...|{1234567890abcdef}|  2024-01-07|\n",
      "|                null|[{43, 79, 2024-01...|{[{2024-01-07T19:...|              null|  2024-01-07|\n",
      "|                null|[{40, 73, 2024-01...|{[{2024-01-05T21:...|{1234567890abcdef}|  2024-01-05|\n",
      "|                null|[{44, 14, 2024-01...|{[{2024-01-05T00:...|              null|  2024-01-05|\n",
      "|                null|[{16, 40, 2024-01...|{[{2024-01-06T17:...|              null|  2024-01-06|\n",
      "|                null|[{37, 44, 2024-01...|{[{2024-01-09T23:...|              null|  2024-01-09|\n",
      "|                null|[{57, 70, 2024-01...|{[{2024-01-10T08:...|              null|  2024-01-10|\n",
      "|                null|[{57, 88, 2024-01...|{[{2024-01-08T00:...|              null|  2024-01-08|\n",
      "|      [ZoneTransfer]|                null|                null|              null|  2024-01-05|\n",
      "|            ZoneId=3|                null|                null|              null|  2024-01-05|\n",
      "|ReferrerUrl=C:\\Us...|                null|                null|              null|  2024-01-05|\n",
      "|      [ZoneTransfer]|                null|                null|              null|  2024-01-08|\n",
      "|            ZoneId=3|                null|                null|              null|  2024-01-08|\n",
      "|ReferrerUrl=C:\\Us...|                null|                null|              null|  2024-01-08|\n",
      "|      [ZoneTransfer]|                null|                null|              null|  2024-01-09|\n",
      "|            ZoneId=3|                null|                null|              null|  2024-01-09|\n",
      "|ReferrerUrl=C:\\Us...|                null|                null|              null|  2024-01-09|\n",
      "|      [ZoneTransfer]|                null|                null|              null|  2024-01-11|\n",
      "|            ZoneId=3|                null|                null|              null|  2024-01-11|\n",
      "+--------------------+--------------------+--------------------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark exibindo as primeiras linhas do DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- author_id: string (nullable = true)\n",
      " |    |    |-- conversation_id: string (nullable = true)\n",
      " |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |-- edit_history_tweet_ids: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- in_reply_to_user_id: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- public_metrics: struct (nullable = true)\n",
      " |    |    |    |-- like_count: long (nullable = true)\n",
      " |    |    |    |-- quote_count: long (nullable = true)\n",
      " |    |    |    |-- reply_count: long (nullable = true)\n",
      " |    |    |    |-- retweet_count: long (nullable = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- includes: struct (nullable = true)\n",
      " |    |-- users: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- username: string (nullable = true)\n",
      " |-- meta: struct (nullable = true)\n",
      " |    |-- next_token: string (nullable = true)\n",
      " |-- extract_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# analisando estrutura do esquema do dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraindo informações de forma simplificada em colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando módulo que permite realizar operações em colunas DataFrames Spark\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col: struct (nullable = true)\n",
      " |    |-- author_id: string (nullable = true)\n",
      " |    |-- conversation_id: string (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- edit_history_tweet_ids: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- in_reply_to_user_id: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- public_metrics: struct (nullable = true)\n",
      " |    |    |-- like_count: long (nullable = true)\n",
      " |    |    |-- quote_count: long (nullable = true)\n",
      " |    |    |-- reply_count: long (nullable = true)\n",
      " |    |    |-- retweet_count: long (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformando data array em linhas e imprimindo a nova estrutura em um formato mais plano para análise\n",
    "df.select(f.explode('data')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 col|\n",
      "+--------------------+\n",
      "|{74, 16, 2024-01-...|\n",
      "|{89, 94, 2024-01-...|\n",
      "|{71, 64, 2024-01-...|\n",
      "|{74, 66, 2024-01-...|\n",
      "|{35, 32, 2024-01-...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retornando os 5 primeiros registros\n",
    "df.select(f.explode('data')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- conversation_id: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- like_count: long (nullable = true)\n",
      " |-- quote_count: long (nullable = true)\n",
      " |-- reply_count: long (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apelidadando a coluna col no dataframe de 'tweets'\n",
    "# pegando cada informação em uma coluna através do segundo select acessando os dados de interesse\n",
    "# utilizando recurso do PySpark '.*' para acessar todos os campos q pertecem à metrics\n",
    "df.select(f.explode(\"data\").alias(\"tweets\"))\\\n",
    ".select(\"tweets.author_id\", \"tweets.conversation_id\",\n",
    "        \"tweets.created_at\", \"tweets.id\",\n",
    "        \"tweets.public_metrics.*\", \"tweets.text\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando novo dataframe em uma variável\n",
    "tweet_df = df.select(f.explode(\"data\").alias(\"tweets\"))\\\n",
    "        .select(\"tweets.author_id\", \"tweets.conversation_id\",\n",
    "        \"tweets.created_at\", \"tweets.id\",\n",
    "        \"tweets.public_metrics.*\", \"tweets.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "|author_id|conversation_id|          created_at| id|like_count|quote_count|reply_count|retweet_count|                text|\n",
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "|       74|             16|2024-01-07T19:19:...| 51|        99|         31|         40|           31|Um terceiro tweet...|\n",
      "|       89|             94|2024-01-07T21:24:...| 83|        49|         76|         92|           16|Outro tweet fictí...|\n",
      "|       71|             64|2024-01-07T12:32:...| 40|         4|         58|         90|            1|Tweet fictício cr...|\n",
      "|       74|             66|2024-01-07T12:55:...| 18|        47|         10|          5|           64|Este é um tweet f...|\n",
      "|       35|             32|2024-01-07T05:34:...| 44|         4|         27|         46|           14|Este é um tweet f...|\n",
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col: struct (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformando users array em linhas e imprimindo a nova estrutura em um formato mais plano para análise\n",
    "df.select(f.explode('includes.users')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[users: struct<created_at:string,id:string,name:string,username:string>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criando um alias de nome coluna 'col' para a novo nome coluna 'users'\n",
    "df.select(f.explode('includes.users').alias('users'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecionando todos os campos de informações dos usuários\n",
    "# transformando os dados em colunas e para primeiro nível do schema\n",
    "df.select(f.explode('includes.users').alias('users')).select('users.*').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando dataframe atualizado em uma nova variável\n",
    "user_df = df.select(f.explode(\"includes.users\").alias(\"users\")).select(\"users.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------+--------+\n",
      "|          created_at| id|  name|username|\n",
      "+--------------------+---+------+--------+\n",
      "|2024-01-07T06:25:...| 35|User 1|   user1|\n",
      "|2024-01-07T17:43:...| 47|User 2|   user2|\n",
      "|2024-01-07T22:37:...| 38|User 3|   user3|\n",
      "|2024-01-07T17:33:...| 88|User 4|   user4|\n",
      "|2024-01-07T02:22:...| 75|User 5|   user5|\n",
      "+--------------------+---+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# visualizando dataframe atualizado\n",
    "user_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unindo as informações do Spark para salvar em um único arquivo\n",
    "# salvando as informações em um arquivo JSON\n",
    "tweet_df.coalesce(1).write.mode(\"overwrite\").json('output/tweet')\n",
    "user_df.coalesce(1).write.mode(\"overwrite\").json('output/user')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
